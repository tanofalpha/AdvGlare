# AdvGlare
AdvGLARE: Generated Lens-based Adversarial Radiant Effect
Recent research has shown that deep neural networks(DNNs), while enjoying tremendous success in computer vision, are highly vulnerable to adversarial perturbations, even naturally occurring ones. While most prior work in physical adversarial attacks relies on artificial artifacts like stickers, projections, or contrived textures, we propose a novel and covert adversarial attack that leverages sun glare, a naturally occurring optical phenomenon. In contrast to methods requiring object access or unrealistic modifications, our attack simulates lens flare effects digitally using warm-colored ghosts, starburst rays, and circular light rings mimicking real world sunlight interference. We fine-tune pretrained image classifiers (ResNet50, DenseNet121, MobileNetV2) on a custom-clean dataset and evaluate them against our glare-perturbed test set. The sun glare generator uses spatial light modeling, Gaussian blurs, and artifact blending to produce adversarial images that remain visually realistic. Our approach demonstrates a significant drop in model performance under glare, reducing classification confidence by around 20-40 percent while preserving perceptual fidelity. These results highlight the susceptibility of DNNs to natural light conditions and the urgent need for robustness against naturally simulated adversarial attacks.
